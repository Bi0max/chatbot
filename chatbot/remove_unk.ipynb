{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "chatbot_path = \"/home/bi0max/projects/tutorials/chatbot\"\n",
    "if chatbot_path not in sys.path:\n",
    "    sys.path.append(chatbot_path)\n",
    "\n",
    "from chatbot.config import *\n",
    "from chatbot.embed_data import *\n",
    "from chatbot import embed_data\n",
    "from chatbot.train import *\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files...\n"
     ]
    }
   ],
   "source": [
    "print(\"Opening files...\")\n",
    "path = os.path.join(DATA_DIR, \"tokenized_from.pickle\")\n",
    "tokenized_from = pickle.load(open(path, \"rb\"))\n",
    "path = os.path.join(DATA_DIR, \"tokenized_to.pickle\")\n",
    "tokenized_to = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "# read matrix, index\n",
    "embedding_matrix = pickle.load(open(PATHS[\"embedding_matrix\"], \"rb\"))\n",
    "word2index = pickle.load(open(PATHS[\"word2index\"], \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate empty sentences\n",
    "def argempty(tokenized_from, tokenized_to):\n",
    "    index_zeros = np.zeros(len(tokenized_from))\n",
    "    for i, sentence_f, sentence_t in zip(range(len(tokenized_from)), tokenized_from, tokenized_to):\n",
    "        if not sentence_f or not sentence_t:\n",
    "            index_zeros[i] = 1\n",
    "    return index_zeros.astype(bool)\n",
    "index_zeros = argempty(tokenized_from, tokenized_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from = np.array(tokenized_from)\n",
    "array_to = np.array(tokenized_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from = array_from[~index_zeros]\n",
    "array_to = array_to[~index_zeros]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3767234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_from.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3767234\n",
      "3767234\n"
     ]
    }
   ],
   "source": [
    "vocab = list(word2index.keys())\n",
    "\n",
    "def argunks(tokenized, vocab, threshold=0.25):\n",
    "    index_uneligible = np.zeros(len(tokenized))\n",
    "    for i in range(tokenized.size):\n",
    "        sentence = tokenized[i]\n",
    "        n_unks = 0\n",
    "        n = len(sentence)\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                n_unks += 1\n",
    "        if (n_unks / n) > threshold:\n",
    "            index_uneligible[i] = 1\n",
    "    return index_uneligible.astype(bool)\n",
    "\n",
    "array_uneligible_to = argunks(array_to, vocab)\n",
    "print(array_uneligible_to.size)\n",
    "array_uneligible_from = argunks(array_from, vocab)\n",
    "print(array_uneligible_from.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176344"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(array_uneligible_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158109"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(array_uneligible_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argunks(array_to[:10], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_uneligible = array_uneligible_from | array_uneligible_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284861"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(array_uneligible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['aplaude', 'para', 'indicarle', 'que', 'ya', 'su', 'discurso', '.']),\n",
       "       list(['yule', '?', 'as', 'in', 'kkkristmas', '?', '?', 'newlinechar', 'fundiealert', 'newlinechar']),\n",
       "       list(['gt', 'an', 'englishman']),\n",
       "       list(['aidens', '.', 'aidens', 'everywhere', '.']),\n",
       "       list(['na', ',', 'elijah', 'radcliffe', '.']),\n",
       "       list(['whiteisright']),\n",
       "       list(['psst', '...', '.wrong', 'place', '.']),\n",
       "       list(['fuckingtruth']), list(['warum', '?']),\n",
       "       list(['me', 'toooo', 'd8'])], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to[array_uneligible_to][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10, 32, 68]),)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_uneligible_to[:100].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10, 46, 55, 58, 70, 92, 94]),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_uneligible_from[:100].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname, obj in zip(['tokenized_from_clean', 'tokenized_to_clean'], \n",
    "                      [array_from[~array_uneligible], array_to[~array_uneligible]]):\n",
    "    path = os.path.join(DATA_DIR, f\"{fname}.pickle\")\n",
    "    pickle.dump(obj, open(path, \"wb\"), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
